# -*- coding: utf-8 -*-
"""genlstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rHhzYLVlP0bKor6ZGj2F3WuWDrirQwTH
"""

import sys
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils
from keras.utils import to_categorical
from keras.models import load_model
from keras.layers import Lambda

# load doc into memory
def load_doc(filename):
	file = open(filename, 'r', encoding='utf-8')
	text = file.read()
	file.close()
	return text

# load the text file
raw_text = load_doc('shakespeare.txt')
raw_text = raw_text.lower()

# map letters to numbers and numbers to letters
chars = sorted(list(set(raw_text)))
char_to_int = dict((c, i) for i, c in enumerate(chars))
int_to_char = dict((i, c) for i, c in enumerate(chars))

# set sequence length to 40
# dataX is input with len 40 sequences
# dataY is next letter in text file
seq_length = 40
n_chars = len(raw_text)
dataX = []
dataY = []

# iterate text file with step size 1
for i in range(0, n_chars - seq_length, 3):
	seq_in = raw_text[i:i + seq_length]
	seq_out = raw_text[i + seq_length]
	dataX.append([char_to_int[char] for char in seq_in])
	dataY.append(char_to_int[seq_out])


vocab_size = len(char_to_int)

# one hot encode input variables
sequences = [to_categorical(x, num_classes=vocab_size) for x in dataX]
X = np.array(sequences)

# one hot encode the output variable
y = np_utils.to_categorical(dataY)

# model with 200 units

model = Sequential()
model.add(LSTM(200,input_shape=(X.shape[1], X.shape[2])))
model.add(Dense(y.shape[1],activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer ='adam',metrics=['accuracy'])

# checkpoint
filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]

model.fit(X, y, epochs=100, batch_size=64, verbose=2, shuffle=True, callbacks=callbacks_list)
scores = model.evaluate(X,y,verbose=1,batch_size=20)
print('Accurracy: {}'.format(scores[1]))

def sample(preds, temp):
    # helper function to sample an index from a probability array
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temp
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds[0], 1)
    return np.argmax(probas)

# initial 40 character seed
seed = "shall i compare thee to a summer's day?\n"
for temperature in [0.25, 0.75, 1.5]:
  print("new temperature")
  # load best weights
  model = Sequential()
  model.add(LSTM(200,input_shape=(X.shape[1], X.shape[2])))
  model.add(Dense(y.shape[1],activation='softmax'))
  model.load_weights("weights.best.hdf5")
  model.compile(loss='categorical_crossentropy', optimizer='adam')

  pattern = [char_to_int[value] for value in seed]
  sys.stdout.write(seed)

  # generate characters
  for i in range(700):
    sequences = [to_categorical(x, num_classes=vocab_size) for x in pattern]
    x = np.array(sequences)
    x = x.reshape(1, x.shape[0], x.shape[1])
    prediction = model.predict(x, verbose=0)
    index = sample(prediction, temperature)
    result = int_to_char[index]
    sys.stdout.write(result)
    pattern.append(index)
    pattern = pattern[1:len(pattern)]

model.save('my_model.h5')

