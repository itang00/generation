{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improvelstm2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "daqlr0J9B7QR",
        "colab_type": "code",
        "outputId": "2665e0e4-b505-4c82-dfac-e33dadb042c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from keras.layers import Lambda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLNEnDLSEFjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r', encoding='utf-8')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbOwYq-YEGi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shakeraw_text = load_doc('shakespeare.txt')\n",
        "shakeraw_text = shakeraw_text.lower()\n",
        "\n",
        "spenraw_text = load_doc('spenser.txt')\n",
        "spenraw_text = spenraw_text.lower()\n",
        "\n",
        "raw_text = shakeraw_text + spenraw_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXrAiMCvLfsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map letters to numbers and numbers to letters\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqAMMIzXNbJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set sequence length to 40\n",
        "# dataX is input with len 40 sequences\n",
        "# dataY is next letter in text file\n",
        "seq_length = 40\n",
        "n_chars = len(raw_text)\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "# iterate text file with step size 3\n",
        "for i in range(0, n_chars - seq_length, 3):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "\n",
        "\n",
        "vocab_size = len(char_to_int)\n",
        "\n",
        "# one hot encode input variables\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in dataX]\n",
        "X = np.array(sequences)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_7lzRYWNduK",
        "colab_type": "code",
        "outputId": "8c7130db-3cb3-42c0-bb50-fb3bd582b5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# model with 200 units\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(200,input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(y.shape[1],activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer ='adam',metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(X, y, epochs=100, batch_size=64, verbose=2, shuffle=True, callbacks=callbacks_list)\n",
        "scores = model.evaluate(X,y,verbose=1,batch_size=20)\n",
        "print('Accurracy: {}'.format(scores[1])) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 52s - loss: 2.5913 - acc: 0.2644\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.59132, saving model to weights.best.hdf5\n",
            "Epoch 2/100\n",
            " - 48s - loss: 2.1277 - acc: 0.3755\n",
            "\n",
            "Epoch 00002: loss improved from 2.59132 to 2.12771, saving model to weights.best.hdf5\n",
            "Epoch 3/100\n",
            " - 48s - loss: 1.9649 - acc: 0.4145\n",
            "\n",
            "Epoch 00003: loss improved from 2.12771 to 1.96495, saving model to weights.best.hdf5\n",
            "Epoch 4/100\n",
            " - 49s - loss: 1.8601 - acc: 0.4416\n",
            "\n",
            "Epoch 00004: loss improved from 1.96495 to 1.86013, saving model to weights.best.hdf5\n",
            "Epoch 5/100\n",
            " - 48s - loss: 1.7824 - acc: 0.4607\n",
            "\n",
            "Epoch 00005: loss improved from 1.86013 to 1.78240, saving model to weights.best.hdf5\n",
            "Epoch 6/100\n",
            " - 49s - loss: 1.7176 - acc: 0.4767\n",
            "\n",
            "Epoch 00006: loss improved from 1.78240 to 1.71756, saving model to weights.best.hdf5\n",
            "Epoch 7/100\n",
            " - 48s - loss: 1.6573 - acc: 0.4915\n",
            "\n",
            "Epoch 00007: loss improved from 1.71756 to 1.65732, saving model to weights.best.hdf5\n",
            "Epoch 8/100\n",
            " - 49s - loss: 1.5985 - acc: 0.5064\n",
            "\n",
            "Epoch 00008: loss improved from 1.65732 to 1.59853, saving model to weights.best.hdf5\n",
            "Epoch 9/100\n",
            " - 49s - loss: 1.5441 - acc: 0.5232\n",
            "\n",
            "Epoch 00009: loss improved from 1.59853 to 1.54405, saving model to weights.best.hdf5\n",
            "Epoch 10/100\n",
            " - 49s - loss: 1.4872 - acc: 0.5367\n",
            "\n",
            "Epoch 00010: loss improved from 1.54405 to 1.48721, saving model to weights.best.hdf5\n",
            "Epoch 11/100\n",
            " - 50s - loss: 1.4276 - acc: 0.5538\n",
            "\n",
            "Epoch 00011: loss improved from 1.48721 to 1.42755, saving model to weights.best.hdf5\n",
            "Epoch 12/100\n",
            " - 50s - loss: 1.3659 - acc: 0.5732\n",
            "\n",
            "Epoch 00012: loss improved from 1.42755 to 1.36594, saving model to weights.best.hdf5\n",
            "Epoch 13/100\n",
            " - 50s - loss: 1.3052 - acc: 0.5903\n",
            "\n",
            "Epoch 00013: loss improved from 1.36594 to 1.30522, saving model to weights.best.hdf5\n",
            "Epoch 14/100\n",
            " - 51s - loss: 1.2362 - acc: 0.6133\n",
            "\n",
            "Epoch 00014: loss improved from 1.30522 to 1.23618, saving model to weights.best.hdf5\n",
            "Epoch 15/100\n",
            " - 51s - loss: 1.1703 - acc: 0.6345\n",
            "\n",
            "Epoch 00015: loss improved from 1.23618 to 1.17029, saving model to weights.best.hdf5\n",
            "Epoch 16/100\n",
            " - 50s - loss: 1.1025 - acc: 0.6573\n",
            "\n",
            "Epoch 00016: loss improved from 1.17029 to 1.10249, saving model to weights.best.hdf5\n",
            "Epoch 17/100\n",
            " - 50s - loss: 1.0346 - acc: 0.6799\n",
            "\n",
            "Epoch 00017: loss improved from 1.10249 to 1.03465, saving model to weights.best.hdf5\n",
            "Epoch 18/100\n",
            " - 50s - loss: 0.9674 - acc: 0.7016\n",
            "\n",
            "Epoch 00018: loss improved from 1.03465 to 0.96735, saving model to weights.best.hdf5\n",
            "Epoch 19/100\n",
            " - 50s - loss: 0.9024 - acc: 0.7251\n",
            "\n",
            "Epoch 00019: loss improved from 0.96735 to 0.90243, saving model to weights.best.hdf5\n",
            "Epoch 20/100\n",
            " - 50s - loss: 0.8404 - acc: 0.7430\n",
            "\n",
            "Epoch 00020: loss improved from 0.90243 to 0.84044, saving model to weights.best.hdf5\n",
            "Epoch 21/100\n",
            " - 50s - loss: 0.7835 - acc: 0.7611\n",
            "\n",
            "Epoch 00021: loss improved from 0.84044 to 0.78350, saving model to weights.best.hdf5\n",
            "Epoch 22/100\n",
            " - 49s - loss: 0.7270 - acc: 0.7802\n",
            "\n",
            "Epoch 00022: loss improved from 0.78350 to 0.72699, saving model to weights.best.hdf5\n",
            "Epoch 23/100\n",
            " - 49s - loss: 0.6788 - acc: 0.7967\n",
            "\n",
            "Epoch 00023: loss improved from 0.72699 to 0.67883, saving model to weights.best.hdf5\n",
            "Epoch 24/100\n",
            " - 49s - loss: 0.6326 - acc: 0.8106\n",
            "\n",
            "Epoch 00024: loss improved from 0.67883 to 0.63258, saving model to weights.best.hdf5\n",
            "Epoch 25/100\n",
            " - 49s - loss: 0.5926 - acc: 0.8248\n",
            "\n",
            "Epoch 00025: loss improved from 0.63258 to 0.59260, saving model to weights.best.hdf5\n",
            "Epoch 26/100\n",
            " - 48s - loss: 0.5528 - acc: 0.8374\n",
            "\n",
            "Epoch 00026: loss improved from 0.59260 to 0.55284, saving model to weights.best.hdf5\n",
            "Epoch 27/100\n",
            " - 49s - loss: 0.5150 - acc: 0.8492\n",
            "\n",
            "Epoch 00027: loss improved from 0.55284 to 0.51496, saving model to weights.best.hdf5\n",
            "Epoch 28/100\n",
            " - 48s - loss: 0.4821 - acc: 0.8598\n",
            "\n",
            "Epoch 00028: loss improved from 0.51496 to 0.48213, saving model to weights.best.hdf5\n",
            "Epoch 29/100\n",
            " - 48s - loss: 0.4591 - acc: 0.8680\n",
            "\n",
            "Epoch 00029: loss improved from 0.48213 to 0.45908, saving model to weights.best.hdf5\n",
            "Epoch 30/100\n",
            " - 50s - loss: 0.4311 - acc: 0.8758\n",
            "\n",
            "Epoch 00030: loss improved from 0.45908 to 0.43114, saving model to weights.best.hdf5\n",
            "Epoch 31/100\n",
            " - 49s - loss: 0.4152 - acc: 0.8799\n",
            "\n",
            "Epoch 00031: loss improved from 0.43114 to 0.41516, saving model to weights.best.hdf5\n",
            "Epoch 32/100\n",
            " - 49s - loss: 0.3886 - acc: 0.8878\n",
            "\n",
            "Epoch 00032: loss improved from 0.41516 to 0.38861, saving model to weights.best.hdf5\n",
            "Epoch 33/100\n",
            " - 50s - loss: 0.3696 - acc: 0.8946\n",
            "\n",
            "Epoch 00033: loss improved from 0.38861 to 0.36959, saving model to weights.best.hdf5\n",
            "Epoch 34/100\n",
            " - 50s - loss: 0.3467 - acc: 0.9026\n",
            "\n",
            "Epoch 00034: loss improved from 0.36959 to 0.34674, saving model to weights.best.hdf5\n",
            "Epoch 35/100\n",
            " - 50s - loss: 0.3350 - acc: 0.9040\n",
            "\n",
            "Epoch 00035: loss improved from 0.34674 to 0.33503, saving model to weights.best.hdf5\n",
            "Epoch 36/100\n",
            " - 48s - loss: 0.3301 - acc: 0.9046\n",
            "\n",
            "Epoch 00036: loss improved from 0.33503 to 0.33013, saving model to weights.best.hdf5\n",
            "Epoch 37/100\n",
            " - 49s - loss: 0.3103 - acc: 0.9124\n",
            "\n",
            "Epoch 00037: loss improved from 0.33013 to 0.31033, saving model to weights.best.hdf5\n",
            "Epoch 38/100\n",
            " - 48s - loss: 0.3021 - acc: 0.9144\n",
            "\n",
            "Epoch 00038: loss improved from 0.31033 to 0.30206, saving model to weights.best.hdf5\n",
            "Epoch 39/100\n",
            " - 48s - loss: 0.2822 - acc: 0.9205\n",
            "\n",
            "Epoch 00039: loss improved from 0.30206 to 0.28222, saving model to weights.best.hdf5\n",
            "Epoch 40/100\n",
            " - 48s - loss: 0.2808 - acc: 0.9193\n",
            "\n",
            "Epoch 00040: loss improved from 0.28222 to 0.28078, saving model to weights.best.hdf5\n",
            "Epoch 41/100\n",
            " - 49s - loss: 0.2716 - acc: 0.9227\n",
            "\n",
            "Epoch 00041: loss improved from 0.28078 to 0.27161, saving model to weights.best.hdf5\n",
            "Epoch 42/100\n",
            " - 49s - loss: 0.2651 - acc: 0.9237\n",
            "\n",
            "Epoch 00042: loss improved from 0.27161 to 0.26509, saving model to weights.best.hdf5\n",
            "Epoch 43/100\n",
            " - 49s - loss: 0.2577 - acc: 0.9259\n",
            "\n",
            "Epoch 00043: loss improved from 0.26509 to 0.25774, saving model to weights.best.hdf5\n",
            "Epoch 44/100\n",
            " - 49s - loss: 0.2574 - acc: 0.9268\n",
            "\n",
            "Epoch 00044: loss improved from 0.25774 to 0.25743, saving model to weights.best.hdf5\n",
            "Epoch 45/100\n",
            " - 47s - loss: 0.2414 - acc: 0.9317\n",
            "\n",
            "Epoch 00045: loss improved from 0.25743 to 0.24141, saving model to weights.best.hdf5\n",
            "Epoch 46/100\n",
            " - 48s - loss: 0.2406 - acc: 0.9297\n",
            "\n",
            "Epoch 00046: loss improved from 0.24141 to 0.24062, saving model to weights.best.hdf5\n",
            "Epoch 47/100\n",
            " - 48s - loss: 0.2295 - acc: 0.9337\n",
            "\n",
            "Epoch 00047: loss improved from 0.24062 to 0.22950, saving model to weights.best.hdf5\n",
            "Epoch 48/100\n",
            " - 49s - loss: 0.2230 - acc: 0.9367\n",
            "\n",
            "Epoch 00048: loss improved from 0.22950 to 0.22299, saving model to weights.best.hdf5\n",
            "Epoch 49/100\n",
            " - 49s - loss: 0.2199 - acc: 0.9364\n",
            "\n",
            "Epoch 00049: loss improved from 0.22299 to 0.21992, saving model to weights.best.hdf5\n",
            "Epoch 50/100\n",
            " - 49s - loss: 0.2159 - acc: 0.9377\n",
            "\n",
            "Epoch 00050: loss improved from 0.21992 to 0.21593, saving model to weights.best.hdf5\n",
            "Epoch 51/100\n",
            " - 48s - loss: 0.2106 - acc: 0.9392\n",
            "\n",
            "Epoch 00051: loss improved from 0.21593 to 0.21064, saving model to weights.best.hdf5\n",
            "Epoch 52/100\n",
            " - 48s - loss: 0.2132 - acc: 0.9386\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.21064\n",
            "Epoch 53/100\n",
            " - 49s - loss: 0.1962 - acc: 0.9434\n",
            "\n",
            "Epoch 00053: loss improved from 0.21064 to 0.19619, saving model to weights.best.hdf5\n",
            "Epoch 54/100\n",
            " - 48s - loss: 0.2013 - acc: 0.9405\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.19619\n",
            "Epoch 55/100\n",
            " - 48s - loss: 0.2066 - acc: 0.9389\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.19619\n",
            "Epoch 56/100\n",
            " - 49s - loss: 0.1961 - acc: 0.9422\n",
            "\n",
            "Epoch 00056: loss improved from 0.19619 to 0.19609, saving model to weights.best.hdf5\n",
            "Epoch 57/100\n",
            " - 48s - loss: 0.1885 - acc: 0.9461\n",
            "\n",
            "Epoch 00057: loss improved from 0.19609 to 0.18855, saving model to weights.best.hdf5\n",
            "Epoch 58/100\n",
            " - 48s - loss: 0.1907 - acc: 0.9440\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.18855\n",
            "Epoch 59/100\n",
            " - 49s - loss: 0.1729 - acc: 0.9505\n",
            "\n",
            "Epoch 00059: loss improved from 0.18855 to 0.17287, saving model to weights.best.hdf5\n",
            "Epoch 60/100\n",
            " - 49s - loss: 0.1852 - acc: 0.9456\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.17287\n",
            "Epoch 61/100\n",
            " - 49s - loss: 0.1878 - acc: 0.9432\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.17287\n",
            "Epoch 62/100\n",
            " - 49s - loss: 0.1658 - acc: 0.9529\n",
            "\n",
            "Epoch 00062: loss improved from 0.17287 to 0.16582, saving model to weights.best.hdf5\n",
            "Epoch 63/100\n",
            " - 49s - loss: 0.1709 - acc: 0.9498\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.16582\n",
            "Epoch 64/100\n",
            " - 50s - loss: 0.1877 - acc: 0.9437\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.16582\n",
            "Epoch 65/100\n",
            " - 50s - loss: 0.1881 - acc: 0.9419\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.16582\n",
            "Epoch 66/100\n",
            " - 47s - loss: 0.1570 - acc: 0.9551\n",
            "\n",
            "Epoch 00066: loss improved from 0.16582 to 0.15696, saving model to weights.best.hdf5\n",
            "Epoch 67/100\n",
            " - 47s - loss: 0.1542 - acc: 0.9562\n",
            "\n",
            "Epoch 00067: loss improved from 0.15696 to 0.15424, saving model to weights.best.hdf5\n",
            "Epoch 68/100\n",
            " - 49s - loss: 0.1792 - acc: 0.9450\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.15424\n",
            "Epoch 69/100\n",
            " - 49s - loss: 0.1723 - acc: 0.9479\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.15424\n",
            "Epoch 70/100\n",
            " - 48s - loss: 0.1639 - acc: 0.9508\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.15424\n",
            "Epoch 71/100\n",
            " - 48s - loss: 0.1506 - acc: 0.9567\n",
            "\n",
            "Epoch 00071: loss improved from 0.15424 to 0.15063, saving model to weights.best.hdf5\n",
            "Epoch 72/100\n",
            " - 48s - loss: 0.1542 - acc: 0.9549\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.15063\n",
            "Epoch 73/100\n",
            " - 47s - loss: 0.1755 - acc: 0.9464\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.15063\n",
            "Epoch 74/100\n",
            " - 47s - loss: 0.1589 - acc: 0.9533\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.15063\n",
            "Epoch 75/100\n",
            " - 46s - loss: 0.1353 - acc: 0.9620\n",
            "\n",
            "Epoch 00075: loss improved from 0.15063 to 0.13531, saving model to weights.best.hdf5\n",
            "Epoch 76/100\n",
            " - 46s - loss: 0.1642 - acc: 0.9504\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.13531\n",
            "Epoch 77/100\n",
            " - 47s - loss: 0.1742 - acc: 0.9451\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.13531\n",
            "Epoch 78/100\n",
            " - 47s - loss: 0.1578 - acc: 0.9523\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.13531\n",
            "Epoch 79/100\n",
            " - 47s - loss: 0.1331 - acc: 0.9625\n",
            "\n",
            "Epoch 00079: loss improved from 0.13531 to 0.13313, saving model to weights.best.hdf5\n",
            "Epoch 80/100\n",
            " - 47s - loss: 0.1394 - acc: 0.9595\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.13313\n",
            "Epoch 81/100\n",
            " - 46s - loss: 0.1563 - acc: 0.9521\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.13313\n",
            "Epoch 82/100\n",
            " - 47s - loss: 0.1550 - acc: 0.9524\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.13313\n",
            "Epoch 83/100\n",
            " - 47s - loss: 0.1455 - acc: 0.9559\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.13313\n",
            "Epoch 84/100\n",
            " - 47s - loss: 0.1301 - acc: 0.9625\n",
            "\n",
            "Epoch 00084: loss improved from 0.13313 to 0.13011, saving model to weights.best.hdf5\n",
            "Epoch 85/100\n",
            " - 47s - loss: 0.1589 - acc: 0.9496\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.13011\n",
            "Epoch 86/100\n",
            " - 47s - loss: 0.1533 - acc: 0.9521\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.13011\n",
            "Epoch 87/100\n",
            " - 47s - loss: 0.1387 - acc: 0.9590\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.13011\n",
            "Epoch 88/100\n",
            " - 47s - loss: 0.1373 - acc: 0.9595\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.13011\n",
            "Epoch 89/100\n",
            " - 46s - loss: 0.1478 - acc: 0.9538\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.13011\n",
            "Epoch 90/100\n",
            " - 47s - loss: 0.1392 - acc: 0.9571\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.13011\n",
            "Epoch 91/100\n",
            " - 47s - loss: 0.1286 - acc: 0.9621\n",
            "\n",
            "Epoch 00091: loss improved from 0.13011 to 0.12861, saving model to weights.best.hdf5\n",
            "Epoch 92/100\n",
            " - 47s - loss: 0.1394 - acc: 0.9567\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.12861\n",
            "Epoch 93/100\n",
            " - 46s - loss: 0.1512 - acc: 0.9521\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.12861\n",
            "Epoch 94/100\n",
            " - 47s - loss: 0.1378 - acc: 0.9578\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.12861\n",
            "Epoch 95/100\n",
            " - 47s - loss: 0.1317 - acc: 0.9594\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.12861\n",
            "Epoch 96/100\n",
            " - 47s - loss: 0.1250 - acc: 0.9627\n",
            "\n",
            "Epoch 00096: loss improved from 0.12861 to 0.12504, saving model to weights.best.hdf5\n",
            "Epoch 97/100\n",
            " - 46s - loss: 0.1355 - acc: 0.9586\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.12504\n",
            "Epoch 98/100\n",
            " - 46s - loss: 0.1523 - acc: 0.9517\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.12504\n",
            "Epoch 99/100\n",
            " - 47s - loss: 0.1400 - acc: 0.9567\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.12504\n",
            "Epoch 100/100\n",
            " - 47s - loss: 0.1198 - acc: 0.9629\n",
            "\n",
            "Epoch 00100: loss improved from 0.12504 to 0.11985, saving model to weights.best.hdf5\n",
            "48489/48489 [==============================] - 61s 1ms/step\n",
            "Accurracy: 0.9703025372886016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiXLyec0PFIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temp):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temp\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds[0], 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9UIAnLKNz0-",
        "colab_type": "code",
        "outputId": "f14f0080-637a-4568-eb5c-dc6805eb565c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# initial 40 character seed\n",
        "seed = \"shall i compare thee to a summer's day?\\n\"\n",
        "for temperature in [0.25, 0.75, 1.5]:\n",
        "  print(\"\\n new temperature \\n\")\n",
        "  # load best weights\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(200,input_shape=(X.shape[1], X.shape[2])))\n",
        "  model.add(Dense(y.shape[1],activation='softmax'))\n",
        "  model.load_weights(\"weights.best.hdf5\")\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "  pattern = [char_to_int[value] for value in seed]\n",
        "  sys.stdout.write(seed)\n",
        "\n",
        "  # generate characters\n",
        "  for i in range(700):\n",
        "    sequences = [to_categorical(x, num_classes=vocab_size) for x in pattern]\n",
        "    x = np.array(sequences)\n",
        "    x = x.reshape(1, x.shape[0], x.shape[1])\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = sample(prediction, temperature)\n",
        "    result = int_to_char[index]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " new temperature \n",
            "\n",
            "shall i compare thee to a summer's day?\n",
            "when thou wilt both now inch me a sail,\n",
            "which with upless and parain from her dight,\n",
            "the winger beauty glornoty simmed flime,\n",
            "whose sulf to be doth being hadd not fon:\n",
            "when in his must enoust fioth heaven asporn,\n",
            "(ar if the such dost parter, which brignt\n",
            "in dart love betught thou did chows dird-and his dreast to be.\n",
            "\n",
            "\n",
            "\n",
            "thou art he sermint of your praises to be,\n",
            "sieden sush abord mad beine are not thee.\n",
            "\n",
            "\n",
            "\n",
            "thy heart love your me ause of wist,\n",
            "so groding the with of mer's light night,\n",
            "and dark from have sweet with thee wist,\n",
            "is but thou braint the sin every will.\n",
            "\n",
            "\n",
            "\n",
            "why have your cold distirity,\n",
            "verpery can mine with self so stornate,\n",
            "then in her praise and chenuly stound,\n",
            "that in her will see\n",
            " new temperature \n",
            "\n",
            "shall i compare thee to a summer's day?\n",
            "when thou well bund his chuse and his pleasure,\n",
            "of the fire of geader prece that true how storl:\n",
            "and wist in heaven ye dy mus cannet pire,\n",
            "the chreppece of thy sour feauty with beauty,\n",
            "me cruel abokn her, in vanghes to dis fant,\n",
            "but thou art hear this givery grain hour fill,\n",
            "make not imin as fingew, tine's base thy wied,\n",
            "me frich herrain fromnclive and endies,\n",
            "these baingest care appear, and fleatily grain.\n",
            "\n",
            "i\n",
            "semmle that with uthavense her, that doth fair find,\n",
            "'wiet live, and in my paris and shady,\n",
            "whose farth her firen, on erse patce queence,\n",
            "then thou will all me nor me be thy sun,\n",
            "git ustewall the tarking looding truth,\n",
            "sidde heartes the chise of love is and art,\n",
            "i cray my played with g\n",
            " new temperature \n",
            "\n",
            "shall i compare thee to a summer's day?\n",
            "swexcivery of my beartion quite would be.\n",
            "\n",
            "\n",
            "\n",
            "in to love, that should loog hath more slyse,\n",
            "for thou of my seefure ambaint) will bright,\n",
            "porforn flownris dary and boon, suce is their dreat,\n",
            "long in them thy uther than his show breas.\n",
            "then in the parily to quelied clowe,\n",
            "her ene perving, they in chason as darth,\n",
            "sine no beholding wandert thine renumbso,\n",
            "of my charg again for deliavent,\n",
            "sink my eme in thing more my love) where my good.\n",
            "gith ye their hus there or aubse pride?\n",
            "in shappy spitt,\n",
            "that fliecous thum you are doth must bring:\n",
            "that mades is my give i can mind fride,\n",
            "or acquit be seekem prey,\n",
            "but not to brows hush of my verever mund,\n",
            "for when frow my self is mind of groder pross.\n",
            "\n",
            "imas t"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFDjRPbLPSkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}